---
name: benchmark tests

on:
  schedule:
    - cron: '0 10 * * 0,3,5,6' # Runs at 10 AM UTC (2AM PT) on Wednesday, Friday, Saturday, and Sunday.
  workflow_dispatch:
    inputs:
      run_all:
        description: 'Run all benchmarks'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'
      bench_name:
        description: 'Benchmark test to run'
        required: false
        default: ''
        type: choice
        options:
          - ''
          - 'all'
          - 'tpch'
          - 'tpcds'
          - 'clickbench'
      selected_benchmark:
        description: 'Individual connector/accelerator benchmarks to run'
        required: false
        default: ''
        type: choice
        options:
          - ''
          - 'spice.ai connector'
          - 's3 connector'
          - 'file connector'
          - 'abfs connector'
          - 'spark connector'
          - 'postgres connector'
          - 'mysql connector'
          - 'duckdb connector'
          - 'odbc-databricks connector'
          - 'odbc-athena connector'
          - 'delta_lake connector'
          - 'arrow accelerator (memory mode)'
          - 'duckdb accelerator (memory mode)'
          - 'duckdb accelerator (file mode)'
          - 'sqlite accelerator (memory mode)'
          - 'sqlite accelerator (file mode)'
          - 'postgres accelerator'
      update_snapshots:
        description: 'Create PR to update snapshots'
        required: false
        default: 'no'
        type: choice
        options:
          - 'always'
          - 'no'

env:
  FEATURES: 'postgres,spark,mysql,odbc,delta_lake,databricks,duckdb,sqlite'

jobs:
  build-database-bench-binary:
    name: Build Benchmark Test Binary
    runs-on: spiceai-large-runners
    steps:
      - uses: actions/checkout@v4

      - name: Set up Rust
        uses: ./.github/actions/setup-rust
        with:
          os: 'linux'

      - name: Install Protoc
        uses: arduino/setup-protoc@v3

      - name: Install the cc tool chain for building benchmark binary
        uses: ./.github/actions/setup-cc

      - name: Build benchmark binary
        run: cargo bench -p runtime --features ${{ env.FEATURES }} --profile release --no-run

      - name: Find, move, and rename benchmark binary
        run: find target/release/deps -type f -name "bench-*" ! -name "*.d" -exec mv {} ./spice_bench \;

      - name: Upload benchmark binary
        uses: actions/upload-artifact@v4
        with:
          name: spice_bench
          path: ./spice_bench

  run-database-bench:
    name: Run ${{ matrix.name }} ${{ matrix.bench }} Benchmark
    runs-on: spiceai-runners
    needs: build-database-bench-binary

    services:
      mysql_tpch:
        image: ${{ matrix.cmd == '-c mysql' && 'ghcr.io/spiceai/spice-mysql-bench:latest' || '' }}
        options: >-
          --health-cmd="mysqladmin ping -uroot -proot -P 3306 --silent"
          --health-interval 60s
          --health-timeout 5s
          --health-retries 20
        ports:
          - 3306:3306
        env:
          MYSQL_ROOT_PASSWORD: root
      mysql_tpcds:
        image: ${{ matrix.cmd == '-c mysql -b tpcds' && 'ghcr.io/spiceai/spice-mysql-tpcds-bench:latest' || '' }}
        options: >-
          --shm-size=10gb
          --health-cmd="mysqladmin ping -uroot -proot -P 3306 --silent"
          --health-interval 60s
          --health-timeout 5s
          --health-retries 20
        ports:
          - 3306:3306
        env:
          MYSQL_ROOT_PASSWORD: root
      postgres_tpch:
        image: ${{ matrix.cmd == '-c postgres' && 'ghcr.io/spiceai/spice-postgres-bench:latest' || '' }}
        options: >-
          --shm-size=2gb
          --health-cmd="test -f /var/lib/postgresql/data/data_loading_complete"
          --health-interval 30s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
        env:
          POSTGRES_PASSWORD: postgres
      postgres_tpcds:
        image: ${{ matrix.cmd == '-c postgres -b tpcds' && 'ghcr.io/spiceai/spice-postgres-tpcds-bench:latest' || '' }}
        options: >-
          --shm-size=2gb
          --health-cmd="test -f /var/lib/postgresql/data/data_loading_complete"
          --health-interval 30s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
        env:
          POSTGRES_PASSWORD: postgres
      postgres_acc:
        image: ${{( matrix.cmd == '-a postgres' || matrix.cmd == '-a postgres -b tpcds') && 'ghcr.io/cloudnative-pg/postgresql:16-bookworm' || '' }}
        options: >-
          --shm-size=2gb
          --health-cmd="pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
        env:
          POSTGRES_PASSWORD: postgres

    strategy:
      fail-fast: false
      max-parallel: 10
      matrix:
        include:
          - cmd: '-c spice.ai'
            name: 'spice.ai connector'
            bench: 'tpch'
          - cmd: '-c s3'
            name: 's3 connector'
            bench: 'tpch'
          - cmd: '-c file'
            name: 'file connector'
            bench: 'tpch'
          - cmd: '-c s3 -b tpcds'
            name: 's3 connector'
            bench: 'tpcds'
          - cmd: '-c abfs'
            name: 'abfs connector'
            bench: 'tpch'
          - cmd: '-c spark'
            name: 'spark connector'
            bench: 'tpch'
          - cmd: '-c postgres'
            name: 'postgres connector'
            bench: 'tpch'
          - cmd: '-c postgres -b tpcds'
            name: 'postgres connector'
            bench: 'tpcds'
          - cmd: '-c mysql'
            name: 'mysql connector'
            bench: 'tpch'
          - cmd: '-c duckdb'
            name: 'duckdb connector'
            bench: 'tpch'
          - cmd: '-c duckdb -b tpcds'
            name: 'duckdb connector'
            bench: 'tpcds'
          - cmd: '-c mysql -b tpcds'
            name: 'mysql connector'
            bench: 'tpcds'
          - cmd: '-c odbc-databricks'
            name: 'odbc-databricks connector'
            bench: 'tpch'
          - cmd: '-c odbc-athena'
            name: 'odbc-athena connector'
            bench: 'tpch'
          - cmd: '-c delta_lake'
            name: 'delta_lake connector'
            bench: 'tpch'
          - cmd: '-c delta_lake -b tpcds'
            name: 'delta_lake connector'
            bench: 'tpcds'
          - cmd: '-a arrow'
            name: 'arrow accelerator (memory mode)'
            bench: 'tpch'
          - cmd: '-a duckdb -m memory'
            name: 'duckdb accelerator (memory mode)'
            bench: 'tpch'
          - cmd: '-a duckdb -m file'
            name: 'duckdb accelerator (file mode)'
            bench: 'tpch'
          - cmd: '-a sqlite -m memory'
            name: 'sqlite accelerator (memory mode)'
            bench: 'tpch'
          - cmd: '-a sqlite -m file'
            name: 'sqlite accelerator (file mode)'
            bench: 'tpch'
          - cmd: '-a postgres'
            name: 'postgres accelerator'
            bench: 'tpch'
          - cmd: '-a arrow -b tpcds'
            name: 'arrow accelerator (memory mode)'
            bench: 'tpcds'
          - cmd: '-a duckdb -m memory -b tpcds'
            name: 'duckdb accelerator (memory mode)'
            bench: 'tpcds'
          - cmd: '-a duckdb -m file -b tpcds'
            name: 'duckdb accelerator (file mode)'
            bench: 'tpcds'
          - cmd: '-a sqlite -m memory -b tpcds'
            name: 'sqlite accelerator (memory mode)'
            bench: 'tpcds'
          - cmd: '-a sqlite -m file -b tpcds'
            name: 'sqlite accelerator (file mode)'
            bench: 'tpcds'
          - cmd: '-a postgres -b tpcds'
            name: 'postgres accelerator'
            bench: 'tpcds'

    steps:
      - name: Checkout repository
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        uses: actions/checkout@v4

      - name: Install required packages
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: sudo apt-get update && sudo apt-get install -y unixodbc-dev unixodbc wget

      - name: Set up Spice.ai API Key
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: |
          echo 'SPICEAI_API_KEY="${{ secrets.SPICE_SECRET_SPICEAI_BENCHMARK_KEY }}"' > .env

      - name: Download benchmark binary
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        uses: actions/download-artifact@v4
        with:
          name: spice_bench

      - name: 'Restart accelerator service container with customize configurations'
        if: ( matrix.cmd == '-a postgres' || matrix.cmd == '-a postgres -b tpcds') && ((github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          docker exec ${{ job.services.postgres_acc.id }} sh -c 'echo "shared_buffers=1GB" >> /var/lib/postgresql/data/postgresql.conf'
          docker exec ${{ job.services.postgres_acc.id }} sh -c 'echo "work_mem=256MB" >> /var/lib/postgresql/data/postgresql.conf'
          docker exec ${{ job.services.postgres_acc.id }} sh -c 'echo "max_wal_size=6GB" >> /var/lib/postgresql/data/postgresql.conf'
          docker kill --signal=SIGHUP ${{ job.services.postgres_acc.id }}

      - name: Install Databricks ODBC driver
        if: matrix.cmd == '-c odbc-databricks' && ((github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          sudo apt-get install unixodbc unixodbc-dev unzip libsasl2-modules-gssapi-mit -y
          wget https://databricks-bi-artifacts.s3.us-east-2.amazonaws.com/simbaspark-drivers/odbc/2.8.2/SimbaSparkODBC-2.8.2.1013-Debian-64bit.zip
          unzip SimbaSparkODBC-2.8.2.1013-Debian-64bit.zip
          sudo dpkg -i simbaspark_2.8.2.1013-2_amd64.deb
          cat <<EOF > /tmp/odbcinst.ini
          [Simba Spark ODBC Driver]
          Description=Simba Spark ODBC Driver
          Driver=/opt/simba/spark/lib/64/libsparkodbc_sb64.so
          Setup=/opt/simba/spark/lib/64/libsparkodbc_sb64.so
          EOF
          sudo cp /tmp/odbcinst.ini /etc/odbcinst.ini

      - name: Install Athena ODBC driver
        if: matrix.cmd == '-c odbc-athena' && ((github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          sudo apt-get install alien -y
          wget https://downloads.athena.us-east-1.amazonaws.com/drivers/ODBC/v2.0.3.0/Linux/AmazonAthenaODBC-2.0.3.0.rpm
          sudo alien -i AmazonAthenaODBC-2.0.3.0.rpm
          cat <<EOF > /tmp/odbcinst.ini
          [Amazon Athena ODBC Driver]
          Description=Amazon Athena ODBC Driver
          Driver=/opt/athena/odbc/lib/libathena-odbc.so
          Setup=/opt/athena/odbc/lib/libathena-odbc.so
          EOF
          sudo cp /tmp/odbcinst.ini /etc/odbcinst.ini

      - name: Make benchmark binary executable
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: chmod +x ./spice_bench

      - name: Download DuckDB Connector Data
        if: matrix.cmd == '-c duckdb' && ((github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/duckdb/tpch.db

      - name: Download DuckDB Connector Data
        if: matrix.cmd == '-c duckdb -b tpcds' && ((github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/duckdb/tpcds.db

      - name: Download File Connector Data
        if: matrix.cmd == '-c file' && ((github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/tpch/customer/customer.parquet
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/tpch/lineitem/lineitem.parquet
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/tpch/nation/nation.parquet
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/tpch/orders/orders.parquet
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/tpch/part/part.parquet
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/tpch/partsupp/partsupp.parquet
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/tpch/region/region.parquet
          wget https://spiceai-public-datasets.s3.us-east-1.amazonaws.com/tpch/supplier/supplier.parquet

      - name: Run benchmark with ${{ matrix.name }}
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: INSTA_WORKSPACE_ROOT="${PWD}" CARGO_MANIFEST_DIR="${PWD}" INSTA_UPDATE=${{ github.event.inputs.update_snapshots}} ./spice_bench --bench ${{ matrix.cmd }}
        continue-on-error: true
        env:
          UPLOAD_RESULTS_DATASET: 'spiceai.tests.oss_benchmarks'
          PG_BENCHMARK_PG_HOST: localhost
          PG_BENCHMARK_PG_USER: postgres
          PG_BENCHMARK_PG_PASS: postgres
          PG_BENCHMARK_PG_SSLMODE: disable
          PG_TPCH_BENCHMARK_PG_DBNAME: tpch_sf1
          PG_TPCDS_BENCHMARK_PG_DBNAME: tpcds_sf1
          PG_BENCHMARK_ACC_PG_DBNAME: postgres
          SPICE_SPARK_REMOTE: ${{ secrets.SPICE_SPARK_REMOTE }}
          MYSQL_BENCHMARK_MYSQL_HOST: localhost
          MYSQL_BENCHMARK_MYSQL_USER: root
          MYSQL_BENCHMARK_MYSQL_PASS: root
          MYSQL_TPCH_BENCHMARK_MYSQL_DB: tpch_sf1
          MYSQL_TPCDS_BENCHMARK_MYSQL_DB: tpcds_sf1
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_ODBC_PATH: ${{ secrets.DATABRICKS_ODBC_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          AWS_DATABRICKS_DELTA_ACCESS_KEY_ID: ${{ secrets.AWS_DATABRICKS_DELTA_ACCESS_KEY_ID }}
          AWS_DATABRICKS_DELTA_SECRET_ACCESS_KEY: ${{ secrets.AWS_DATABRICKS_DELTA_SECRET_ACCESS_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_S3_ATHENA_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_S3_ATHENA_SECRET_ACCESS_KEY }}

      - name: Remove spice_bench file
        run: |
          rm -f spice_bench

      - name: Upload benchmark snapshots to branch
        if: github.event.inputs.update_snapshots == 'always'
        run: |
          git config --global user.name 'Spice Benchmark Snapshot Update Bot'
          git config --global user.email 'spiceaibot@spice.ai'
          git checkout -b bench-snapshot-update/${{ github.run_id }}
          git add '*.snap'
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update benchmark snapshots for ${{ matrix.name }} ${{ matrix.bench }}"
            if git ls-remote --exit-code --heads origin bench-snapshot-update/${{ github.run_id }}; then
              git pull --rebase origin bench-snapshot-update/${{ github.run_id }}
            fi
            git push origin bench-snapshot-update/${{ github.run_id }}
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  run-clickbench:
    name: Run ${{ matrix.name }} Clickbench
    runs-on: spiceai-large-runners
    needs: build-database-bench-binary

    strategy:
      fail-fast: false
      max-parallel: 1
      matrix:
        include:
          - cmd: '-a arrow -b clickbench'
            name: 'arrow accelerator (memory mode)'
            bench: 'clickbench'
          - cmd: '-a duckdb -m file -b clickbench'
            name: 'duckdb accelerator (file mode)'
            bench: 'clickbench'
          - cmd: '-a duckdb -m memory -b clickbench'
            name: 'duckdb accelerator (memory mode)'
            bench: 'clickbench'

    steps:
      - name: Checkout repository
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        uses: actions/checkout@v4

      - name: Install required packages
        run: sudo apt-get update && sudo apt-get install -y unixodbc-dev unixodbc wget

      - name: Set up Spice.ai API Key
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: |
          echo 'SPICEAI_API_KEY="${{ secrets.SPICE_SECRET_SPICEAI_BENCHMARK_KEY }}"' > .env

      - name: Download benchmark binary
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        uses: actions/download-artifact@v4
        with:
          name: spice_bench

      - name: Make benchmark binary executable
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: chmod +x ./spice_bench

      - name: Run benchmark with ${{ matrix.name }}
        if: (github.event.inputs.selected_benchmark == matrix.name && (github.event.inputs.bench_name == matrix.bench || github.event.inputs.bench_name == 'all')) || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: INSTA_WORKSPACE_ROOT="${PWD}" CARGO_MANIFEST_DIR="${PWD}" ./spice_bench --bench ${{ matrix.cmd }}
        continue-on-error: true
        env:
          UPLOAD_RESULTS_DATASET: 'spiceai.tests.oss_benchmarks'
          CLICKBENCH_S3_ENDPOINT: ${{ secrets.CLICKBENCH_S3_ENDPOINT}}
          CLICKBENCH_S3_KEY: ${{ secrets.CLICKBENCH_S3_KEY}}
          CLICKBENCH_S3_SECRET: ${{ secrets.CLICKBENCH_S3_SECRET}}

  create-pr:
    runs-on: ubuntu-latest
    needs: run-database-bench
    if: always() && github.event.inputs.update_snapshots == 'always'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Create PR
        run: |
          git config --global user.name 'Spice Benchmark Snapshot Update Bot'
          git config --global user.email 'spiceaibot@spice.ai'
          git fetch origin bench-snapshot-update/${{ github.run_id }}
          git checkout bench-snapshot-update/${{ github.run_id }}
          gh pr create --title "Update benchmark snapshots" --body "Updated benchmark snapshots" --base trunk --head bench-snapshot-update/${{ github.run_id }}
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
